import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import os
import argparse

from tqdm import tqdm

from autoencoder.autoencoder import Autoencoder
from autoencoder_latent_resnet.latent_resnet import LatentResNetClassifier


def train_classifier(args):
    """
    Trains the ResNetClassifier on the latent space representations
    generated by the pre-trained encoder.
    """
    device = torch.device("mps")
    print(f"Using device: {device}")

    # 1. Load the pre-trained and frozen encoder
    print(f"Loading pre-trained encoder from '{args.autoencoder_path}'...")
    try:
        autoencoder_model = Autoencoder(in_channels=3)
        autoencoder_model.load_state_dict(
            torch.load(args.autoencoder_path, map_location=device)
        )
        encoder = autoencoder_model.encoder.to(device)
    except FileNotFoundError:
        print(f"Error: Autoencoder model not found at '{args.autoencoder_path}'")
        return

    # Freeze the encoder's parameters so they are not updated during training
    for param in encoder.parameters():
        param.requires_grad = False
    encoder.eval()
    print("Encoder loaded and frozen.")

    # 2. Set up data loaders
    image_size = 256  # Should match the autoencoder's training
    transform = transforms.Compose(
        [
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
        ]
    )

    train_dir = os.path.join(args.dataset_path, "train")
    valid_dir = os.path.join(args.dataset_path, "val")

    if not os.path.isdir(train_dir) or not os.path.isdir(valid_dir):
        print(
            f"Error: 'train' or 'valid' directories not found in '{args.dataset_path}'."
        )
        print(
            "Please ensure your dataset is structured with 'train' and 'valid' subdirectories."
        )
        return

    train_dataset = datasets.ImageFolder(train_dir, transform=transform)
    valid_dataset = datasets.ImageFolder(valid_dir, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False)

    num_classes = len(train_dataset.classes)
    print(f"Found {num_classes} classes: {train_dataset.classes}")

    # 3. Initialize the classifier model, optimizer, and loss function
    classifier = LatentResNetClassifier(num_classes=num_classes, in_channels=192).to(
        device
    )
    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)
    criterion = nn.CrossEntropyLoss()
    print("Classifier model initialized.")

    # 4. Start the training and validation loop
    print("\n--- Starting Classifier Training ---")
    best_accuracy = 0.0
    for epoch in range(args.epochs):
        # --- Training Phase ---
        classifier.train()
        train_loss = 0.0
        for images, labels in tqdm(
            train_loader, desc=f"Training Epoch {epoch + 1}/{args.epochs}"
        ):
            images, labels = images.to(device), labels.to(device)

            # Generate latent space representations with the frozen encoder
            with torch.no_grad():
                latent_tensors = encoder(images)

            # Forward pass through the classifier
            optimizer.zero_grad()
            outputs = classifier(latent_tensors)

            # Calculate loss and update classifier weights
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # --- Validation Phase ---
        classifier.eval()
        valid_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in valid_loader:
                images, labels = images.to(device), labels.to(device)

                latent_tensors = encoder(images)
                outputs = classifier(latent_tensors)

                loss = criterion(outputs, labels)
                valid_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        avg_train_loss = train_loss / len(train_loader)
        avg_valid_loss = valid_loss / len(valid_loader)
        accuracy = 100 * correct / total

        print(f"--- End of Epoch [{epoch + 1}/{args.epochs}] ---")
        print(f"    Avg. Training Loss: {avg_train_loss:.4f}")
        print(f"    Avg. Validation Loss: {avg_valid_loss:.4f}")
        print(f"    Validation Accuracy: {accuracy:.2f}%")

        # Checkpoint the best model
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(
                classifier.state_dict(), "checkpoints/best_latent_resnet_classifier.pth"
            )
            print(f"    New best model saved with accuracy: {accuracy:.2f}%")

    print("\n--- Training Finished ---")
    print(f"Best validation accuracy achieved: {best_accuracy:.2f}%")
    print("Best model saved to 'checkpoints/best_latent_resnet_classifier.pth'")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Train a classifier on the latent space of a pre-trained encoder."
    )
    parser.add_argument(
        "--dataset_path",
        type=str,
        default="/Users/easonni/Desktop/Coding/science fair/FusedTensorImageClassifier/dataset",
        help="Path to the root of the dataset (containing 'train' and 'valid' subdirectories).",
    )
    parser.add_argument(
        "--autoencoder_path",
        type=str,
        default="../checkpoints/best_autoencoder.pth",
        help="Path to the trained autoencoder model file (.pth).",
    )
    parser.add_argument(
        "--epochs", type=int, default=20, help="Number of training epochs."
    )
    parser.add_argument(
        "--batch_size", type=int, default=16, help="Batch size for training."
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-3,
        help="Learning rate for the optimizer.",
    )

    args = parser.parse_args()
    train_classifier(args)
